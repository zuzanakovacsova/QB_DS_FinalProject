{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis\n",
    "## Running some simple models first\n",
    "\n",
    "![alt text](https://scikit-learn.org/stable/_static/ml_map.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up environment\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from IPython.display import Math\n",
    " \n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,VotingClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn import metrics, svm,tree,preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import (auc, confusion_matrix, roc_curve, accuracy_score, precision_score)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>diagnosis_category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>842302</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>842517</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84300903</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84348301</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84358402</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "id                                                               \n",
       "842302          17.99         10.38          122.80     1001.0   \n",
       "842517          20.57         17.77          132.90     1326.0   \n",
       "84300903        19.69         21.25          130.00     1203.0   \n",
       "84348301        11.42         20.38           77.58      386.1   \n",
       "84358402        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "          smoothness_mean  compactness_mean  concavity_mean  \\\n",
       "id                                                            \n",
       "842302            0.11840           0.27760          0.3001   \n",
       "842517            0.08474           0.07864          0.0869   \n",
       "84300903          0.10960           0.15990          0.1974   \n",
       "84348301          0.14250           0.28390          0.2414   \n",
       "84358402          0.10030           0.13280          0.1980   \n",
       "\n",
       "          concave points_mean  symmetry_mean  fractal_dimension_mean  ...  \\\n",
       "id                                                                    ...   \n",
       "842302                0.14710         0.2419                 0.07871  ...   \n",
       "842517                0.07017         0.1812                 0.05667  ...   \n",
       "84300903              0.12790         0.2069                 0.05999  ...   \n",
       "84348301              0.10520         0.2597                 0.09744  ...   \n",
       "84358402              0.10430         0.1809                 0.05883  ...   \n",
       "\n",
       "          texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "id                                                                       \n",
       "842302            17.33           184.60      2019.0            0.1622   \n",
       "842517            23.41           158.80      1956.0            0.1238   \n",
       "84300903          25.53           152.50      1709.0            0.1444   \n",
       "84348301          26.50            98.87       567.7            0.2098   \n",
       "84358402          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "          compactness_worst  concavity_worst  concave points_worst  \\\n",
       "id                                                                   \n",
       "842302               0.6656           0.7119                0.2654   \n",
       "842517               0.1866           0.2416                0.1860   \n",
       "84300903             0.4245           0.4504                0.2430   \n",
       "84348301             0.8663           0.6869                0.2575   \n",
       "84358402             0.2050           0.4000                0.1625   \n",
       "\n",
       "          symmetry_worst  fractal_dimension_worst  diagnosis_category  \n",
       "id                                                                     \n",
       "842302            0.4601                  0.11890                   1  \n",
       "842517            0.2750                  0.08902                   1  \n",
       "84300903          0.3613                  0.08758                   1  \n",
       "84348301          0.6638                  0.17300                   1  \n",
       "84358402          0.2364                  0.07678                   1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Uploading data\n",
    "df_processed= pd.read_csv(\"/Suzi fun files/QB course/QB_DS_FinalProject/data/processed/data_all_features.csv\", index_col=[0])\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into test and train\n",
    "target = df_processed['diagnosis_category']\n",
    "X = df_processed.drop('diagnosis_category',axis = 1)\n",
    "X = X.values\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, target, test_size = 0.2,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 31)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First algorithm: decision tree\n",
    "- suitable for data with collinearity\n",
    "- can be visualised \n",
    "- our dataset is quite small so computational complexity not a big problem \n",
    "\n",
    "I score my grid search with recall, as I want high recall in tumour diagnosis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune Decision Tree\n",
    "num_leaves = [5, 10, 15, 20, 30, 40, 50]\n",
    "depth = np.arange(3, 21)\n",
    "num_samples_split = np.arange(10,20)\n",
    "params_DT = {\"max_depth\": depth, \"min_samples_leaf\": num_leaves, \"min_samples_split\":num_samples_split}\n",
    "random_state = 42\n",
    "classifier_DT = DecisionTreeClassifier(random_state=random_state)\n",
    "grid_DT = GridSearchCV(classifier_DT, params_DT,scoring='roc_auc');\n",
    "grid_DT.fit(X_train,y_train)\n",
    "\n",
    "prediction_DT = grid_DT.predict(X_test)\n",
    "probability_DT = grid_DT.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_DT_df = pd.DataFrame(grid_DT.cv_results_)\n",
    "grid_DT_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, prediction_DT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DT= grid_DT.best_estimator_\n",
    "plt.figure(figsize=(20,10)) \n",
    "tree.plot_tree(model_DT, feature_names=df_processed.columns[0:-1], impurity=False, proportion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.confusion_matrix(y_test, prediction_DT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_tn, tree_fn, tree_fp, tree_tp = metrics.confusion_matrix(y_test, prediction_DT).ravel()\n",
    "recall_DT= tree_tp/(tree_tp + tree_fn)\n",
    "print(recall_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(grid_DT_df[['mean_test_score', 'param_max_depth']].groupby('param_max_depth').mean(), \n",
    "         marker='o', linestyle='None', markersize = 10.0)\n",
    "plt.xlabel('Depth of tree')\n",
    "plt.ylabel('Average testing score (recall)')\n",
    "plt.title('Average testing score according to the depth of the tree')\n",
    "\n",
    "print(grid_DT.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's my decision tree done.\n",
    "## Let's try another non-parametric algorithm, support vector machines. \n",
    "\n",
    "For SVM, I need to normalise my data for zero mean and unit variance. \n",
    "\n",
    "Some info on SVM https://statinfer.com/204-6-8-svm-advantages-disadvantages-applications/\n",
    "\n",
    "Application to this dataset https://towardsdatascience.com/breast-cancer-classification-using-support-vector-machine-svm-a510907d4878\n",
    "\n",
    "Tuning inspired by https://medium.com/@aneesha/svm-parameter-tuning-in-scikit-learn-using-gridsearchcv-2413c02125a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = preprocessing.scale(X_train)\n",
    "X_test_scaled  = preprocessing.scale(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune support vector machine\n",
    "kernels = ['linear','poly','rbf']\n",
    "Cs = [0.01, 0.1, 1, 10, 100]\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "params_SVM = {\"kernel\": ['linear'], \"C\":Cs}\n",
    "{\"kernel\" : ['poly'],\"gamma\": gammas, \"C\":Cs}\n",
    "{\"kernel\" : ['rbf'],\"gamma\": gammas, \"C\":Cs}\n",
    "classifier_SVM = svm.SVC(probability=True)\n",
    "grid_SVM = GridSearchCV(classifier_SVM,params_SVM,scoring='roc_auc')\n",
    "grid_SVM.fit(X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_SVM_df = pd.DataFrame(grid_SVM.cv_results_)\n",
    "grid_SVM_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_SVM = grid_SVM.predict(X_test_scaled)\n",
    "probability_SVM = grid_SVM.predict_proba(X_test_scaled)\n",
    "print(grid_SVM.best_params_)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, prediction_SVM))\n",
    "print(metrics.confusion_matrix(y_test, prediction_SVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_tn, svm_fn, svm_fp, svm_tp = metrics.confusion_matrix(y_test, prediction_SVM).ravel()\n",
    "recall_SVM = svm_tp/(svm_tp + svm_fn)\n",
    "print(recall_SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As my SVM model uses a linear kernel, I can assess feature importance\n",
    "taken from https://stackoverflow.com/questions/41592661/determining-the-most-contributing-features-for-svm-classifier-in-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_importances(coef,estimator, names, top=-1):\n",
    "    imp = coef\n",
    "    imp, names = zip(*sorted(list(zip(imp, names))))\n",
    "\n",
    "    if top == -1:\n",
    "        top = len(names)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.barh(range(top), imp[::-1][0:top], align='center')\n",
    "    plt.yticks(range(top), names[::-1][0:top])\n",
    "    plt.title(\"Feature importance in {}.\".format(estimator))\n",
    "    plt.show()\n",
    "\n",
    "features_names = list(df_processed.columns)    \n",
    "model_SVM = grid_SVM.best_estimator_\n",
    "f_importances(abs(model_SVM.coef_[0]),\"SVM\", features_names, top=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-nearest neighbours, popular with this dataset\n",
    "See https://www.geeksforgeeks.org/ml-kaggle-breast-cancer-wisconsin-diagnosis-using-knn/ or *Sarkar M, Leong TY. Application of K-nearest neighbors algorithm on breast cancer diagnosis problem. Proc AMIA Symp. 2000;759â€“763.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune KNN\n",
    "neighbours = [5,10,15,20]\n",
    "power_parameter = [1,2]\n",
    "params_KNN = {\"n_neighbors\": neighbours, \"p\": power_parameter}\n",
    "classifier_KNN = KNeighborsClassifier()\n",
    "grid_KNN = GridSearchCV(classifier_KNN,params_KNN,scoring='roc_auc')\n",
    "grid_KNN.fit(X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_KNN = grid_KNN.predict(X_test_scaled)\n",
    "probability_KNN = grid_KNN.predict_proba(X_test_scaled)\n",
    "print(grid_KNN.best_params_)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, prediction_KNN))\n",
    "print(metrics.confusion_matrix(y_test, prediction_KNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_tn, knn_fn, knn_fp, knn_tp = metrics.confusion_matrix(y_test, prediction_KNN).ravel()\n",
    "recall_KNN = knn_tp/(knn_tp + knn_fn)\n",
    "print(recall_KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple ensemble model: combine my KNN and SVM\n",
    "using the best parameters from my grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_KNN= grid_KNN.best_estimator_\n",
    "model_SVM= grid_SVM.best_estimator_\n",
    "classifier_SVC_KNN = VotingClassifier(estimators = [('knn',model_KNN),('svm',model_SVM)],voting = 'soft')\n",
    "classifier_SVC_KNN.fit(X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_SVC_KNN = classifier_SVC_KNN.predict(X_test_scaled)\n",
    "probability_SVC_KNN = classifier_SVC_KNN.predict_proba(X_test_scaled)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, prediction_SVC_KNN))\n",
    "print(metrics.confusion_matrix(y_test, prediction_SVC_KNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_knn_tn, svc_knn_fn, svc_knn_fp, svc_knn_tp = metrics.confusion_matrix(y_test, prediction_SVC_KNN).ravel()\n",
    "recall_SVC_KNN = svc_knn_tp/(svc_knn_tp + svc_knn_fn)\n",
    "print(recall_SVC_KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune RF\n",
    "n_estimators = [int(x) for x in np.linspace(start = 50, stop = 150, num = 11)]\n",
    "max_features = ['auto', 'sqrt','log2']\n",
    "max_depth = [int(x) for x in np.linspace(5, 55, num = 11)]\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [10, 20, 30, 40]\n",
    "bootstrap = [True,False]\n",
    "params_RF = {'n_estimators': n_estimators,'max_features':max_features,'max_depth':max_depth,\n",
    "             'min_samples_split':min_samples_split,'min_samples_leaf':min_samples_leaf,'bootstrap':bootstrap}\n",
    "classifier_RF = RandomForestClassifier()\n",
    "grid_RF = RandomizedSearchCV(classifier_RF,params_RF,n_iter = 300,scoring='roc_auc',random_state=random_state,n_jobs = -1,cv=5,verbose=2)\n",
    "grid_RF.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_RF =grid_RF.predict(X_test)\n",
    "probability_RF = grid_RF.predict_proba(X_test)\n",
    "print(grid_RF.best_params_)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, prediction_RF))\n",
    "print(metrics.confusion_matrix(y_test, prediction_RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tn, rf_fn, rf_fp, rf_tp = metrics.confusion_matrix(y_test, prediction_RF).ravel()\n",
    "recall_RF = rf_tp/(rf_tp + rf_fn)\n",
    "print(recall_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest feature importance\n",
    "https://blog.datadive.net/selecting-good-features-part-iii-random-forests/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (\"Features sorted by their score(decrease of impurity):\")\n",
    "print (sorted(zip(map(lambda x: round(x, 4), grid_RF.best_estimator_.feature_importances_), features_names), \n",
    "             reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f_importances(abs(grid_RF.best_estimator_.feature_importances_),\"Random Forest\", features_names, top=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting: AdaBoost\n",
    "mostly used on Decision trees https://machinelearningmastery.com/boosting-and-adaboost-for-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_AB = AdaBoostClassifier(base_estimator=None, n_estimators=40,random_state=random_state)\n",
    "classifier_AB.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_AB = classifier_AB.predict(X_test)\n",
    "probability_AB = classifier_AB.predict_proba(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, prediction_AB))\n",
    "print(metrics.confusion_matrix(y_test, prediction_AB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_tn, ab_fn, ab_fp, ab_tp = metrics.confusion_matrix(y_test, prediction_AB).ravel()\n",
    "recall_AB = ab_tp/(ab_tp + ab_fn)\n",
    "print(recall_AB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting\n",
    "tuning: https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning with same parameters as my random forest\n",
    "learning_rate = [0.1, 0.2, 0.3];\n",
    "params_GB = {'learning_rate': learning_rate,'n_estimators': n_estimators,'max_features':max_features,'max_depth':max_depth,\n",
    "             'min_samples_split':min_samples_split,'min_samples_leaf':min_samples_leaf}\n",
    "classifier_GB = GradientBoostingClassifier()\n",
    "grid_GB = RandomizedSearchCV(classifier_GB,params_GB,n_iter = 300,scoring='roc_auc',random_state=random_state,n_jobs = -1,cv=5,verbose=2)\n",
    "grid_GB.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_GB = grid_GB.predict(X_test)\n",
    "probability_GB = grid_GB.predict_proba(X_test)\n",
    "print(grid_GB.best_params_)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, prediction_GB))\n",
    "print(metrics.confusion_matrix(y_test, prediction_GB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_tn, gb_fn, gb_fp, gb_tp = metrics.confusion_matrix(y_test, prediction_GB).ravel()\n",
    "recall_GB = gb_tp/(gb_tp + gb_fn)\n",
    "print(recall_GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_importances(abs(grid_GB.best_estimator_.feature_importances_),\"Gradient Boosting\", features_names, top=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pos_rate_DT,true_pos_rate_DT, thresholds_DT = roc_curve(y_test, probability_DT[:, 1])\n",
    "false_pos_rate_SVM,true_pos_rate_SVM, thresholds_SVM = roc_curve(y_test, probability_SVM[:, 1])\n",
    "false_pos_rate_KNN,true_pos_rate_KNN, thresholds_KNN = roc_curve(y_test, probability_KNN[:, 1])\n",
    "false_pos_rate_SVC_KNN,true_pos_rate_SVC_KNN, thresholds_SVC_KNN = roc_curve(y_test, probability_SVC_KNN[:, 1])\n",
    "false_pos_rate_RF,true_pos_rate_RF, thresholds_RF = roc_curve(y_test, probability_RF[:, 1])\n",
    "false_pos_rate_AB,true_pos_rate_AB, thresholds_AB = roc_curve(y_test, probability_AB[:, 1])\n",
    "false_pos_rate_GB,true_pos_rate_GB, thresholds_GB = roc_curve(y_test, probability_GB[:, 1])\n",
    "\n",
    "# Plot the ROC curve\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(false_pos_rate_DT, true_pos_rate_DT, label='DT')\n",
    "plt.plot(false_pos_rate_SVM, true_pos_rate_SVM, label='SVM')\n",
    "plt.plot(false_pos_rate_KNN, true_pos_rate_KNN, label='KNN')\n",
    "plt.plot(false_pos_rate_SVC_KNN, true_pos_rate_SVC_KNN, label='SVC/KNN')\n",
    "plt.plot(false_pos_rate_RF, true_pos_rate_RF, label='RF')\n",
    "plt.plot(false_pos_rate_AB, true_pos_rate_AB, label='AB')\n",
    "plt.plot(false_pos_rate_GB, true_pos_rate_GB, label='GB')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(thresholds_DT)\n",
    "print(thresholds_SVM)\n",
    "print(thresholds_KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"Accuracy DT:\",metrics.accuracy_score(y_test, prediction_DT)*100,\"Recall DT:\",recall_DT*100)\n",
    "print(\"Accuracy SVM:\",metrics.accuracy_score(y_test, prediction_SVM)*100,\"Recall SVM:\",recall_SVM*100)\n",
    "print(\"Accuracy KNN:\",metrics.accuracy_score(y_test, prediction_KNN)*100,\"Recall KNN:\",recall_KNN*100)\n",
    "print(\"Accuracy SVC/KNN:\",metrics.accuracy_score(y_test, prediction_SVC_KNN)*100,\"Recall SVC/KNN:\",recall_SVC_KNN*100)\n",
    "print(\"Accuracy RF:\",metrics.accuracy_score(y_test, prediction_RF)*100,\"Recall RF:\",recall_RF*100)\n",
    "print(\"Accuracy AB:\",metrics.accuracy_score(y_test, prediction_AB)*100,\"Recall AB:\",recall_AB*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save my test and train data for use in different notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)\n",
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train.to_csv(\"/Suzi fun files/QB course/QB_DS_FinalProject/data/interim/y_train.csv\")\n",
    "#y_test.to_csv(\"/Suzi fun files/QB course/QB_DS_FinalProject/data/interim/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(X_test).to_csv(\"/Suzi fun files/QB course/QB_DS_FinalProject/data/interim/X_test.csv\")\n",
    "#pd.DataFrame(X_train).to_csv(\"/Suzi fun files/QB course/QB_DS_FinalProject/data/interim/X_train.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
